Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
10000,1.3741857,52.31147540983606,0.04413552,0.04756830613697558,0.04756830613697558,0.003677203,0.23559637,0.00029650095,1.0
20000,1.3202688,55.30708661417323,0.0638906,0.13216850433564392,0.13216850433564392,0.0038616653,0.24499801,0.00029084858,1.0
30000,1.2585348,194.37254901960785,0.010543092,0.01647333310150619,0.01647333310150619,0.00078316405,0.2467049,0.0002849888,1.0
40000,1.2410945,226.08823529411765,0.045402072,0.29884167012722984,0.29884167012722984,0.0022636515,0.23577131,0.00027885064,1.0
50000,1.1026565,182.43636363636364,0.34348488,0.921625457563972,0.921625457563972,0.0056443796,0.25128147,0.00027291375,1.0
60000,0.8615917,78.4517766497462,0.44819832,0.4193833341426701,0.4193833341426701,0.009772154,0.25738075,0.00026625636,1.0
70000,0.7721149,27.36285714285714,0.29882607,0.30384970601768985,0.30384970601768985,0.02018621,0.24554057,0.00026078452,1.0
