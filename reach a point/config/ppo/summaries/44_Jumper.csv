Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
10000,1.3487114,95.5,-0.08618612,0.03970106435047049,0.03970106435047049,0.0041107005,0.24762861,0.00029695092,1.0
20000,1.303676,95.4059405940594,-0.0031740966,-0.002952525093345997,-0.002952525093345997,3.1592874e-05,0.24611889,0.00029103042,1.0
30000,1.2814208,85.91803278688525,-0.006388645,-0.002636363493877055,-0.002636363493877055,2.2854447e-05,0.25185955,0.00028479734,1.0
40000,1.3260939,112.3529411764706,0.06543685,0.26696621765920614,0.26696621765920614,0.0040393025,0.2452914,0.00027900338,1.0
50000,1.0257566,59.48598130841121,0.6821913,0.9777944149585163,0.9777944149585163,0.0062569147,0.2570356,0.00027283255,1.0
60000,0.5688728,23.28354430379747,0.7110944,0.7339149942468632,0.7339149942468632,0.01571187,0.25292635,0.00026643064,1.0
70000,0.8832122,52.42780748663102,0.28177723,0.3488225987545112,0.3488225987545112,0.012478555,0.23800062,0.0002611784,1.0
80000,0.8229771,33.57854406130268,0.2936024,0.26127163032528034,0.26127163032528034,0.019263653,0.25124973,0.00025519868,1.0
